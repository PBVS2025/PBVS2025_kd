{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_class_files(root_dir: str) -> Dict[str, Dict[str, Set[str]]]:\n",
    "    \"\"\"\n",
    "    각 클래스별로 EO와 SAR에 공통으로 존재하는 파일들을 수집합니다.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[class_name, Dict['eo_files'|'sar_files', Set[filenames]]]\n",
    "    \"\"\"\n",
    "    class_files = defaultdict(lambda: {'eo_files': set(), 'sar_files': set()})\n",
    "    \n",
    "    # EO 파일 수집\n",
    "    eo_path = os.path.join(root_dir, 'EO_Train')\n",
    "    for class_name in os.listdir(eo_path):\n",
    "        class_path = os.path.join(eo_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_files[class_name]['eo_files'] = set(os.listdir(class_path))\n",
    "    \n",
    "    # SAR 파일 수집\n",
    "    sar_path = os.path.join(root_dir, 'SAR_Train')\n",
    "    for class_name in os.listdir(sar_path):\n",
    "        class_path = os.path.join(sar_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_files[class_name]['sar_files'] = set(os.listdir(class_path))\n",
    "    \n",
    "    return class_files\n",
    "\n",
    "def create_directory_structure(output_dir: str, class_names: List[str]):\n",
    "    \"\"\"지정된 디렉토리 구조를 생성합니다.\"\"\"\n",
    "    for split in ['train', 'val']:\n",
    "        for domain in ['EO_Train', 'SAR_Train']:\n",
    "            for class_name in class_names:\n",
    "                os.makedirs(os.path.join(output_dir, split, domain, class_name), exist_ok=True)\n",
    "\n",
    "def copy_files(\n",
    "    root_dir: str,\n",
    "    output_dir: str,\n",
    "    class_name: str,\n",
    "    files: List[str],\n",
    "    split: str\n",
    "):\n",
    "    \"\"\"파일들을 지정된 위치로 복사합니다.\"\"\"\n",
    "    for file_name in files:\n",
    "        # EO 파일 복사\n",
    "        shutil.copy2(\n",
    "            os.path.join(root_dir, 'EO_Train', class_name, file_name),\n",
    "            os.path.join(output_dir, split, 'EO_Train', class_name, file_name)\n",
    "        )\n",
    "        \n",
    "        # SAR 파일 복사\n",
    "        shutil.copy2(\n",
    "            os.path.join(root_dir, 'SAR_Train', class_name, file_name),\n",
    "            os.path.join(output_dir, split, 'SAR_Train', class_name, file_name)\n",
    "        )\n",
    "\n",
    "def create_balanced_split_dataset(\n",
    "    root_dir: str,\n",
    "    output_dir: str,\n",
    "    samples_per_class: int = 350,  # semi truck with trailer 기준\n",
    "    train_ratio: float = 0.8,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Balanced dataset을 생성하고 train/val로 분할합니다.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: 원본 데이터셋 경로\n",
    "        output_dir: 생성될 데이터셋 경로\n",
    "        samples_per_class: 각 클래스당 선택할 총 샘플 수\n",
    "        train_ratio: 학습 데이터 비율 (0과 1 사이)\n",
    "        seed: 랜덤 시드\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 클래스별 파일 수집\n",
    "    class_files = get_class_files(root_dir)\n",
    "    \n",
    "    # 디렉토리 구조 생성\n",
    "    create_directory_structure(output_dir, list(class_files.keys()))\n",
    "    \n",
    "    print(\"\\n=== 클래스별 샘플링 및 분할 통계 ===\")\n",
    "    print(f\"{'클래스명':<25} {'원본 파일 수':>12} {'선택된 총 샘플 수':>15} {'학습':>8} {'검증':>8}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    total_stats = {'original': 0, 'selected': 0, 'train': 0, 'val': 0}\n",
    "    \n",
    "    # 각 클래스별로 처리\n",
    "    for class_name, files in class_files.items():\n",
    "        # EO와 SAR에 모두 존재하는 파일만 선택\n",
    "        common_files = files['eo_files'] & files['sar_files']\n",
    "        \n",
    "        # 요청된 수만큼 랜덤 샘플링\n",
    "        n_samples = min(samples_per_class, len(common_files))\n",
    "        selected_files = random.sample(list(common_files), n_samples)\n",
    "        \n",
    "        # train/val 분할\n",
    "        n_train = int(n_samples * train_ratio)\n",
    "        train_files = selected_files[:n_train]\n",
    "        val_files = selected_files[n_train:]\n",
    "        \n",
    "        # 파일 복사\n",
    "        copy_files(root_dir, output_dir, class_name, train_files, 'train')\n",
    "        copy_files(root_dir, output_dir, class_name, val_files, 'val')\n",
    "        \n",
    "        # 통계 출력\n",
    "        print(f\"{class_name:<25} {len(common_files):>12} {n_samples:>15} {len(train_files):>8} {len(val_files):>8}\")\n",
    "        \n",
    "        # 전체 통계 업데이트\n",
    "        total_stats['original'] += len(common_files)\n",
    "        total_stats['selected'] += n_samples\n",
    "        total_stats['train'] += len(train_files)\n",
    "        total_stats['val'] += len(val_files)\n",
    "    \n",
    "    # 전체 통계 출력\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'전체':<25} {total_stats['original']:>12} {total_stats['selected']:>15} {total_stats['train']:>8} {total_stats['val']:>8}\")\n",
    "\n",
    "def main():\n",
    "    root_dir = \"/home/whisper2024/PBVS/Unicorn_Dataset\"  # 원본 데이터셋 경로\n",
    "    output_dir = \"/home/whisper2024/PBVS/Unicorn_Split_Dataset\"  # 생성될 데이터셋 경로\n",
    "    \n",
    "    create_balanced_split_dataset(\n",
    "        root_dir=root_dir,\n",
    "        output_dir=output_dir,\n",
    "        samples_per_class=350,  # semi truck with trailer 기준\n",
    "        train_ratio=0.8,  # 8:2 비율로 분할\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_fixed_samples(base_dir, samples_per_class=30, seed=42):\n",
    "    \"\"\"\n",
    "    각 클래스에서 정확히 samples_per_class 개수만큼의 샘플을 추출하여 검증 세트로 이동합니다.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): 데이터셋의 기본 디렉토리 경로 (train 폴더를 포함하는 경로)\n",
    "        samples_per_class (int): 각 클래스에서 추출할 샘플 수\n",
    "        seed (int): 랜덤 시드\n",
    "    \"\"\"\n",
    "    # 경로 설정\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    val_dir = os.path.join(base_dir, \"val\")\n",
    "    \n",
    "    # SAR와 EO 폴더 경로\n",
    "    train_sar_dir = os.path.join(train_dir, \"SAR_Train\")\n",
    "    train_eo_dir = os.path.join(train_dir, \"EO_Train\")\n",
    "    \n",
    "    # 검증 폴더 생성\n",
    "    val_sar_dir = os.path.join(val_dir, \"SAR_Train\")\n",
    "    val_eo_dir = os.path.join(val_dir, \"EO_Train\")\n",
    "    \n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(val_sar_dir, exist_ok=True)\n",
    "    os.makedirs(val_eo_dir, exist_ok=True)\n",
    "    \n",
    "    # 랜덤 시드 설정\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 클래스 목록\n",
    "    classes = [d for d in os.listdir(train_sar_dir) if os.path.isdir(os.path.join(train_sar_dir, d))]\n",
    "    \n",
    "    print(f\"데이터셋 분리 시작. 각 클래스에서 {samples_per_class}개 샘플 추출\")\n",
    "    print(f\"처리할 클래스: {classes}\")\n",
    "    \n",
    "    # 각 클래스별 통계\n",
    "    stats = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        print(f\"\\n클래스 '{class_name}' 처리 중...\")\n",
    "        \n",
    "        # SAR 이미지 목록\n",
    "        sar_images = glob.glob(os.path.join(train_sar_dir, class_name, \"*\"))\n",
    "        \n",
    "        # EO 이미지 경로\n",
    "        train_eo_class_dir = os.path.join(train_eo_dir, class_name)\n",
    "        \n",
    "        # Paired 이미지 찾기 (SAR와 EO 모두 존재하는 이미지)\n",
    "        paired_images = []\n",
    "        \n",
    "        for sar_path in sar_images:\n",
    "            filename = os.path.basename(sar_path)\n",
    "            eo_path = os.path.join(train_eo_class_dir, filename)\n",
    "            \n",
    "            if os.path.exists(eo_path):\n",
    "                paired_images.append(filename)\n",
    "        \n",
    "        print(f\"  - 총 이미지 수: {len(sar_images)}\")\n",
    "        print(f\"  - Paired 이미지 수: {len(paired_images)}\")\n",
    "        \n",
    "        # 검증 세트로 이동할 이미지 선택 (paired 이미지에서 samples_per_class개 선택)\n",
    "        if len(paired_images) >= samples_per_class:\n",
    "            val_images = random.sample(paired_images, samples_per_class)\n",
    "            print(f\"  - 검증 세트로 이동할 이미지 수: {len(val_images)}\")\n",
    "        else:\n",
    "            val_images = paired_images\n",
    "            print(f\"  - 경고: 요청한 {samples_per_class}개보다 적은 {len(val_images)}개의 이미지만 가능함\")\n",
    "        \n",
    "        # 검증 세트를 위한 폴더 생성\n",
    "        val_sar_class_dir = os.path.join(val_sar_dir, class_name)\n",
    "        val_eo_class_dir = os.path.join(val_eo_dir, class_name)\n",
    "        \n",
    "        os.makedirs(val_sar_class_dir, exist_ok=True)\n",
    "        os.makedirs(val_eo_class_dir, exist_ok=True)\n",
    "        \n",
    "        # 검증 세트로 이미지 이동\n",
    "        for filename in tqdm(val_images, desc=f\"{class_name} 이동 중\"):\n",
    "            # SAR 이미지 이동\n",
    "            src_sar_path = os.path.join(train_sar_dir, class_name, filename)\n",
    "            dst_sar_path = os.path.join(val_sar_class_dir, filename)\n",
    "            shutil.move(src_sar_path, dst_sar_path)\n",
    "            \n",
    "            # EO 이미지 이동\n",
    "            src_eo_path = os.path.join(train_eo_dir, class_name, filename)\n",
    "            dst_eo_path = os.path.join(val_eo_class_dir, filename)\n",
    "            shutil.move(src_eo_path, dst_eo_path)\n",
    "        \n",
    "        # 통계 저장\n",
    "        stats[class_name] = {\n",
    "            \"total\": len(sar_images),\n",
    "            \"paired\": len(paired_images),\n",
    "            \"train\": len(paired_images) - len(val_images),\n",
    "            \"val\": len(val_images)\n",
    "        }\n",
    "    \n",
    "    # 최종 통계 출력\n",
    "    print(\"\\n======== 데이터셋 분리 완료 ========\")\n",
    "    print(\"\\n클래스별 통계:\")\n",
    "    total_train, total_val = 0, 0\n",
    "    \n",
    "    for class_name, class_stats in stats.items():\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  - 학습 세트: {class_stats['train']} 이미지\")\n",
    "        print(f\"  - 검증 세트: {class_stats['val']} 이미지\")\n",
    "        total_train += class_stats['train']\n",
    "        total_val += class_stats['val']\n",
    "    \n",
    "    print(\"\\n전체 통계:\")\n",
    "    print(f\"  - 총 학습 세트: {total_train} 이미지\")\n",
    "    print(f\"  - 총 검증 세트: {total_val} 이미지\")\n",
    "    print(f\"  - 총 이미지: {total_train + total_val} 이미지\")\n",
    "    print(f\"  - 검증 세트 평균 크기: {total_val / len(classes):.1f} 이미지/클래스\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 기본 디렉토리 경로 (이 경로는 'train' 폴더를 포함하는 경로여야 함)\n",
    "    base_dir = \"/home/whisper2024/PBVS/resampled\"\n",
    "    \n",
    "    # 각 클래스에서 추출할 샘플 수\n",
    "    samples_per_class = 30\n",
    "    \n",
    "    extract_fixed_samples(base_dir, samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "box_truck 클래스 처리 중... 총 2896개 이미지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - box_truck:   0%|          | 0/2896 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - box_truck: 100%|██████████| 2896/2896 [00:01<00:00, 1656.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box_truck DBSCAN 파라미터: eps=0.4, min_samples=5\n",
      "box_truck 클래스: 7개 클러스터, 21개 노이즈 포인트\n",
      "box_truck 필터링 결과: 2896개 -> 28개 (0.97%)\n",
      "\n",
      "bus 클래스 처리 중... 총 612개 이미지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - bus: 100%|██████████| 612/612 [00:00<00:00, 1581.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus DBSCAN 파라미터: eps=0.4, min_samples=5\n",
      "bus 클래스: 3개 클러스터, 6개 노이즈 포인트\n",
      "bus 필터링 결과: 612개 -> 9개 (1.47%)\n",
      "\n",
      "flatbed_truck 클래스 처리 중... 총 898개 이미지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - flatbed_truck: 100%|██████████| 898/898 [00:00<00:00, 1638.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatbed_truck DBSCAN 파라미터: eps=0.4, min_samples=5\n",
      "flatbed_truck 클래스: 3개 클러스터, 15개 노이즈 포인트\n",
      "flatbed_truck 필터링 결과: 898개 -> 18개 (2.00%)\n",
      "\n",
      "motorcycle 클래스 처리 중... 총 1441개 이미지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - motorcycle: 100%|██████████| 1441/1441 [00:00<00:00, 1597.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motorcycle DBSCAN 파라미터: eps=0.4, min_samples=5\n",
      "motorcycle 클래스: 11개 클러스터, 39개 노이즈 포인트\n",
      "motorcycle 필터링 결과: 1441개 -> 50개 (3.47%)\n",
      "\n",
      "pickup_truck 클래스 처리 중... 총 24158개 이미지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - pickup_truck: 100%|██████████| 24158/24158 [00:14<00:00, 1631.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_truck DBSCAN 파라미터: eps=0.4, min_samples=5\n",
      "pickup_truck 클래스: 14개 클러스터, 144개 노이즈 포인트\n",
      "pickup_truck 필터링 결과: 24158개 -> 158개 (0.65%)\n",
      "\n",
      "pickup_truck_w_trailer 클래스 처리 중... 총 695개 이미지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - pickup_truck_w_trailer: 100%|██████████| 695/695 [00:00<00:00, 1604.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_truck_w_trailer DBSCAN 파라미터: eps=0.4, min_samples=5\n",
      "pickup_truck_w_trailer 클래스: 4개 클러스터, 10개 노이즈 포인트\n",
      "pickup_truck_w_trailer 필터링 결과: 695개 -> 14개 (2.01%)\n",
      "\n",
      "sedan 클래스 처리 중... 총 364291개 이미지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "특성 추출 - sedan:   7%|▋         | 26412/364291 [00:16<03:28, 1617.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 157\u001b[0m\n\u001b[1;32m    152\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mcopy2(src_path, dst_path)\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 필터링 결과: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개 -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개 (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_indices)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(image_files)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mfilter_sar_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/whisper2024/PBVS/Unicorn_Dataset/EO_Train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/whisper2024/PBVS/Unicorn_Dataset/EO_Train_Filtered\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbox_truck\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflatbed_truck\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmotorcycle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickup_truck\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickup_truck_w_trailer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msedan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msemi_w_trailer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSUV\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_samples_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m, in \u001b[0;36mfilter_sar_dataset\u001b[0;34m(base_dir, output_dir, target_classes, eps_values, min_samples_values)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_file \u001b[38;5;129;01min\u001b[39;00m tqdm(image_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m특성 추출 - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     90\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, img_file)\n\u001b[0;32m---> 91\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_sar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(feature)\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mextract_features_from_sar\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 통계적 특성\u001b[39;00m\n\u001b[1;32m     26\u001b[0m features\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     27\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(img),\n\u001b[1;32m     28\u001b[0m     np\u001b[38;5;241m.\u001b[39mstd(img),\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     30\u001b[0m     np\u001b[38;5;241m.\u001b[39mpercentile(img, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m     31\u001b[0m     np\u001b[38;5;241m.\u001b[39mpercentile(img, \u001b[38;5;241m90\u001b[39m)\n\u001b[1;32m     32\u001b[0m ])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 질감 특성 (GLCM)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/numpy/lib/function_base.py:3856\u001b[0m, in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3774\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_median_dispatcher)\n\u001b[1;32m   3775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, overwrite_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3776\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3777\u001b[0m \u001b[38;5;124;03m    Compute the median along the specified axis.\u001b[39;00m\n\u001b[1;32m   3778\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3854\u001b[0m \n\u001b[1;32m   3855\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_median\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3857\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/numpy/lib/function_base.py:3752\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3749\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[1;32m   3750\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[0;32m-> 3752\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/numpy/lib/function_base.py:3906\u001b[0m, in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3902\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(indexer)\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;66;03m# Use mean in both odd and even case to coerce data type,\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m \u001b[38;5;66;03m# using out array if needed.\u001b[39;00m\n\u001b[0;32m-> 3906\u001b[0m rout \u001b[38;5;241m=\u001b[39m \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[38;5;66;03m# Check if the array contains any nan's\u001b[39;00m\n\u001b[1;32m   3908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(a\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minexact) \u001b[38;5;129;01mand\u001b[39;00m sz \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3909\u001b[0m     \u001b[38;5;66;03m# If nans are possible, warn and replace by nans like mean would.\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/numpy/core/_methods.py:169\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    165\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[1;32m    167\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m rcount \u001b[38;5;241m=\u001b[39m \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    171\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/numpy/core/_methods.py:78\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[1;32m     77\u001b[0m         items \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[mu\u001b[38;5;241m.\u001b[39mnormalize_axis_index(ax, arr\u001b[38;5;241m.\u001b[39mndim)]\n\u001b[0;32m---> 78\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[43mnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstride_tricks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m broadcast_to\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def extract_features_from_sar(image_path):\n",
    "    \"\"\"SAR 이미지에서 특성 추출\"\"\"\n",
    "    # 이미지 로드\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    # 이미지 리사이징 (옵션)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    \n",
    "    # 특성 추출 방법 1: 이미지 자체를 평탄화\n",
    "    # features = img.flatten() / 255.0\n",
    "    \n",
    "    # 특성 추출 방법 2: 통계적 특성 + 질감 특성\n",
    "    features = []\n",
    "    # 통계적 특성\n",
    "    features.extend([\n",
    "        np.mean(img),\n",
    "        np.std(img),\n",
    "        np.median(img),\n",
    "        np.percentile(img, 10),\n",
    "        np.percentile(img, 90)\n",
    "    ])\n",
    "    \n",
    "    # 질감 특성 (GLCM)\n",
    "    try:\n",
    "        from skimage.feature import graycomatrix, graycoprops\n",
    "        glcm = graycomatrix(img, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], 256, symmetric=True, normed=True)\n",
    "        features.extend([\n",
    "            graycoprops(glcm, 'contrast').mean(),\n",
    "            graycoprops(glcm, 'dissimilarity').mean(),\n",
    "            graycoprops(glcm, 'homogeneity').mean(),\n",
    "            graycoprops(glcm, 'energy').mean(),\n",
    "            graycoprops(glcm, 'correlation').mean()\n",
    "        ])\n",
    "    except:\n",
    "        # skimage가 없는 경우 간단한 대체 특성\n",
    "        for i in range(5):\n",
    "            features.append(0)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def filter_sar_dataset(base_dir, output_dir, target_classes=['sedan', 'SUV', 'pickup_truck', 'van'],\n",
    "                      eps_values=[0.3, 0.4, 0.4, 0.4], min_samples_values=[5, 5, 5, 5]):\n",
    "    \"\"\"\n",
    "    SAR 데이터셋에서 유사 샘플 필터링\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : SAR 데이터셋 기본 디렉토리\n",
    "    output_dir : 필터링된 데이터셋을 저장할 디렉토리\n",
    "    target_classes : 필터링할 클래스 리스트\n",
    "    eps_values : 각 클래스별 DBSCAN eps 값\n",
    "    min_samples_values : 각 클래스별 DBSCAN min_samples 값\n",
    "    \"\"\"\n",
    "    # 출력 디렉토리 생성\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 각 클래스별로 처리\n",
    "    for i, class_name in enumerate(target_classes):\n",
    "        class_dir = os.path.join(base_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"경로를 찾을 수 없음: {class_dir}\")\n",
    "            continue\n",
    "        \n",
    "        output_class_dir = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(output_class_dir, exist_ok=True)\n",
    "        \n",
    "        # 이미지 파일 목록 가져오기\n",
    "        image_files = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png', '.tif'))]\n",
    "        print(f\"\\n{class_name} 클래스 처리 중... 총 {len(image_files)}개 이미지\")\n",
    "        \n",
    "        if len(image_files) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 특성 추출\n",
    "        features = []\n",
    "        valid_files = []\n",
    "        \n",
    "        for img_file in tqdm(image_files, desc=f\"특성 추출 - {class_name}\"):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            feature = extract_features_from_sar(img_path)\n",
    "            \n",
    "            if feature is not None:\n",
    "                features.append(feature)\n",
    "                valid_files.append(img_file)\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(f\"{class_name} 클래스에서 유효한 특성을 추출할 수 없음\")\n",
    "            continue\n",
    "        \n",
    "        # 특성 배열로 변환\n",
    "        features = np.array(features)\n",
    "        \n",
    "        # 특성 정규화\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # DBSCAN 파라미터 설정\n",
    "        eps = eps_values[i] if i < len(eps_values) else eps_values[-1]\n",
    "        min_samples = min_samples_values[i] if i < len(min_samples_values) else min_samples_values[-1]\n",
    "        \n",
    "        print(f\"{class_name} DBSCAN 파라미터: eps={eps}, min_samples={min_samples}\")\n",
    "        \n",
    "        # DBSCAN 적용\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = dbscan.fit_predict(features_scaled)\n",
    "        \n",
    "        # 클러스터 통계\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "        print(f\"{class_name} 클래스: {n_clusters}개 클러스터, {n_noise}개 노이즈 포인트\")\n",
    "        \n",
    "        # 선택된 샘플 인덱스\n",
    "        selected_indices = []\n",
    "        \n",
    "        # 노이즈 포인트는 모두 선택\n",
    "        noise_indices = np.where(cluster_labels == -1)[0]\n",
    "        selected_indices.extend(noise_indices)\n",
    "        \n",
    "        # 각 클러스터에서 중심에 가장 가까운 샘플 선택\n",
    "        for cluster_id in set(cluster_labels):\n",
    "            if cluster_id == -1:\n",
    "                continue\n",
    "                \n",
    "            # 현재 클러스터의 포인트들\n",
    "            cluster_points_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "            cluster_points = features_scaled[cluster_points_indices]\n",
    "            \n",
    "            # 클러스터 중심 계산\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            # 중심에서 가장 가까운 포인트 찾기\n",
    "            distances = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            closest_point_idx = cluster_points_indices[np.argmin(distances)]\n",
    "            \n",
    "            selected_indices.append(closest_point_idx)\n",
    "        \n",
    "        # 선택된 이미지 복사\n",
    "        for idx in selected_indices:\n",
    "            src_path = os.path.join(class_dir, valid_files[idx])\n",
    "            dst_path = os.path.join(output_class_dir, valid_files[idx])\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "        \n",
    "        print(f\"{class_name} 필터링 결과: {len(image_files)}개 -> {len(selected_indices)}개 ({len(selected_indices)/len(image_files)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "filter_sar_dataset(\n",
    "    base_dir='/home/whisper2024/PBVS/Unicorn_Dataset/EO_Train',\n",
    "    output_dir='/home/whisper2024/PBVS/Unicorn_Dataset/EO_Train_Filtered',\n",
    "    target_classes=['box_truck',  'bus' , 'flatbed_truck' , 'motorcycle'  ,'pickup_truck' , 'pickup_truck_w_trailer'  ,'sedan'  ,'semi_w_trailer'  ,'SUV'  ,'van'],\n",
    "    eps_values=[0.4, 0.4,0.4, 0.4,0.4, 0.4,0.4, 0.4,],\n",
    "    min_samples_values=[5, 5,5, 5,5, 5,5, 5,5, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
